{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5WNyHfKvnhR",
        "outputId": "3b526020-68e3-4568-b859-bc5f9949342d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHO54IXuvuho"
      },
      "source": [
        "IDS2017URL = \"/gdrive/MyDrive/IDS2017/IDS2017.csv\"\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm0QQ9MAv7RT"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from numpy import vstack\n",
        "from numpy import argmax\n",
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch.nn import Linear\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Softmax\n",
        "from torch.nn import Module\n",
        "from torch.optim import SGD\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "from torch.nn.init import xavier_uniform_"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l2BPkI9u-O6"
      },
      "source": [
        "\n",
        "class IDSDataset(Dataset):\n",
        "\n",
        "    def __init__(self, IDS2017URL):\n",
        "      df = pd.read_csv(IDS2017URL)\n",
        "      df = df[~df['Label'].isin(['Heartbleed', 'Web Attack - Sql Injection', 'Infiltration', 'Web Attack - XSS', 'Web Attack - Brute Force'])] \n",
        "      self.X = df.values[:, :-2]\n",
        "      self.y = df.values[:, -2]\n",
        "      self.X = self.X.astype('float32')\n",
        "      self.y = LabelEncoder().fit_transform(self.y)\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      return [self.X[idx], self.y[idx]]\n",
        " \n",
        "    # get indexes for train and test rows\n",
        "    def get_splits(self, n_test=0.33):\n",
        "        # determine sizes\n",
        "        test_size = round(n_test * len(self.X))\n",
        "        train_size = len(self.X) - test_size\n",
        "        # calculate the split\n",
        "        return random_split(self, [train_size, test_size])\n",
        "\n",
        "class MLP(Module):\n",
        "    # define model elements\n",
        "    def __init__(self, n_inputs):\n",
        "        super(MLP, self).__init__()\n",
        "        # input to first hidden layer\n",
        "        self.hidden1 = Linear(n_inputs, 84)\n",
        "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
        "        self.act1 = ReLU()\n",
        "        # second hidden layer\n",
        "        self.hidden2 = Linear(84, 84)\n",
        "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
        "        self.act2 = ReLU()\n",
        "        # third hidden layer and output\n",
        "        self.hidden3 = Linear(84, 10)\n",
        "        xavier_uniform_(self.hidden3.weight)\n",
        "        self.act3 = Softmax(dim=1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # input to first hidden layer\n",
        "        X = self.hidden1(X)\n",
        "        X = self.act1(X)\n",
        "        # second hidden layer\n",
        "        X = self.hidden2(X)\n",
        "        X = self.act2(X)\n",
        "        # output layer\n",
        "        X = self.hidden3(X)\n",
        "        X = self.act3(X)\n",
        "        return X\n",
        " \n",
        "def prepare_data(path):\n",
        "    # load the dataset\n",
        "    dataset = IDSDataset(path)\n",
        "    # calculate split\n",
        "    train, test = dataset.get_splits()\n",
        "    # prepare data loaders\n",
        "    train_dl = DataLoader(train, batch_size=32000, shuffle=True)\n",
        "    test_dl = DataLoader(test, batch_size=10240, shuffle=False)\n",
        "    return train_dl, test_dl\n",
        "\n",
        "def train_model(train_dl, model):\n",
        "    # define the optimization\n",
        "    criterion = CrossEntropyLoss()\n",
        "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    # enumerate epochs\n",
        "    for epoch in range(10):\n",
        "        # enumerate mini batches\n",
        "        print(epoch)\n",
        "        for i, (inputs, targets) in enumerate(train_dl):\n",
        "            # clear the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # compute the model output\n",
        "            yhat = model(inputs)\n",
        "            # calculate loss\n",
        "            loss = criterion(yhat, targets)\n",
        "            # credit assignment\n",
        "            loss.backward()\n",
        "            # update model weights\n",
        "            optimizer.step()\n",
        "\n",
        "def evaluate_model(test_dl, model):\n",
        "    predictions, actuals = list(), list()\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        # evaluate the model on the test set\n",
        "        yhat = model(inputs)\n",
        "        # retrieve numpy array\n",
        "        yhat = yhat.detach().numpy()\n",
        "        actual = targets.numpy()\n",
        "        # convert to class labels\n",
        "        yhat = argmax(yhat, axis=1)\n",
        "        # reshape for stacking\n",
        "        actual = actual.reshape((len(actual), 1))\n",
        "        yhat = yhat.reshape((len(yhat), 1))\n",
        "        # store\n",
        "        predictions.append(yhat)\n",
        "        actuals.append(actual)\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
        "    # calculate accuracy\n",
        "    acc = accuracy_score(actuals, predictions)\n",
        "    return acc\n",
        "\n",
        "def predict(row, model):\n",
        "    # convert row to data\n",
        "    row = Tensor([row])\n",
        "    # make prediction\n",
        "    yhat = model(row)\n",
        "    # retrieve numpy array\n",
        "    yhat = yhat.detach().numpy()\n",
        "    return yhat"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K93DyQu42DyP",
        "outputId": "f62cf394-9df6-4380-a6d3-ccdc60a55bb2"
      },
      "source": [
        "train_dl, test_dl = prepare_data(IDS2017URL)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (84) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9MC8dvBWxD0",
        "outputId": "9bf7476b-72be-4d76-d8f7-58fa7f9483d0"
      },
      "source": [
        "train_dl.dataset"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataset.Subset at 0x7f741df07160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTmP4irw2VTu",
        "outputId": "26f41253-6a6b-4973-a881-9259f877a05b"
      },
      "source": [
        "print(len(train_dl.dataset), len(test_dl.dataset))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1895106 933410\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE1Uy8t23IoV"
      },
      "source": [
        "model = MLP(83)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YRRGIyH3LGW",
        "outputId": "78b286ad-ff44-4105-a2ac-32b304f6b3a3"
      },
      "source": [
        "train_model(train_dl, model)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Edbu4O05lCY"
      },
      "source": [
        "torch.save(model.state_dict(), '/gdrive/MyDrive/IDS2017/1.pth')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqQF9d57SmOC",
        "outputId": "90a7630d-7449-4160-b131-8d5afbc5d6d7"
      },
      "source": [
        "acc = evaluate_model(test_dl, model)\n",
        "print('Accuracy: %.3f' % acc)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMd6a2J0WcpZ",
        "outputId": "1af1365c-73bf-49c1-93d9-3974bdcb24bb"
      },
      "source": [
        "print(model.state_dict())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('hidden1.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]])), ('hidden1.bias', tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])), ('hidden2.weight', tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]])), ('hidden2.bias', tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])), ('hidden3.weight', tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])), ('hidden3.bias', tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]))])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lywJkqvXUwCr",
        "outputId": "1f5d41ed-5f5c-43c8-bd73-5b8ece3dfab5"
      },
      "source": [
        "row = [24049,1541,49633,2610,80,6,264,5125872,8,7,1659,2514.0,467,0,207.375,221.2780652,1047,0,359.14285710000007,420.18386,814.0,2.0,366133.7143,1320784.35,4954614.0,73.0,171258.0,24465.42857,21945.246219999997,57169.0,1001.0,5125743.0,854290.5,2033515.304,5004981.0,2049.0,0,0,0,0,172,152,1.560710061,1.3656213030000002,0,1047,260.8125,322.5548468,104041.6292,0,0,0,1,0,0,0,0,0,278.2,207.375,359.14285710000007,0,0,0,0,0,0,8,1659,7,2514,8192,262,7,20,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
        "yhat = predict(row, model)\n",
        "print(yhat)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[nan nan nan nan nan nan nan nan nan nan]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPbLUthSWPG8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}